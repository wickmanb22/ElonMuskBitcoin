filter(author == "satoshi") %>%
right_join(stopword_freq %>%
filter(author == "satoshi") %>%
filter(row_number() < 7), by = "word") %>%
ggplot() +
geom_density(aes(per1000, group = word, color = word, fill = word), alpha = 0.3) +
theme_clean() +
labs(
title = "Rates of Use of Satoshi's Most Frequent Words"
)
# Most popular stopwords
(stopword_freq <- data %>%
group_by(author, word) %>%
summarise(total_usage = sum(freq)) %>%
pivot_wider(names_from = "author", values_from = "total_usage") %>%
arrange(desc(satoshi)))
# Most popular stopwords
(stopword_freq <- data %>%
group_by(author, word) %>%
summarise(total_usage = sum(freq)) %>%
pivot_wider(names_from = "author", values_from = "total_usage") %>%
arrange(desc(satoshi)) %>%
print(n = 12))
print(n = 12)
print(stopword_freq, n = 12)
# Most popular stopwords
(stopword_freq <- data %>%
group_by(author, word) %>%
summarise(total_usage = sum(freq)) %>%
pivot_wider(names_from = "author", values_from = "total_usage") %>%
arrange(desc(musk)) %>%
print(n = 12))
# Most popular stopwords
(stopword_freq <- data %>%
group_by(author, word) %>%
summarise(total_usage = sum(freq)) %>%
pivot_wider(names_from = "author", values_from = "total_usage") %>%
arrange(desc(musk)))
print(stopword_freq)
print(stopword_freq, 12)
print(stopword_freq, n = 12)
# Most popular stopwords
(stopword_freq <- data %>%
group_by(author, word) %>%
summarise(total_usage = sum(freq)) %>%
pivot_wider(names_from = "author", values_from = "total_usage") %>%
arrange(desc(musk))) %>%
rename(nakamoto = satoshi)
print(stopword_freq, n = 12)
# Most popular stopwords
(stopword_freq <- data %>%
group_by(author, word) %>%
summarise(total_usage = sum(freq)) %>%
pivot_wider(names_from = "author", values_from = "total_usage") %>%
arrange(desc(musk))) %>%
mutate(total_use = musk + satoshi) %>%
rename(nakamoto = satoshi)
print(stopword_freq, n = 12)
# Most popular stopwords
(stopword_freq <- data %>%
group_by(author, word) %>%
summarise(total_usage = sum(freq)) %>%
pivot_wider(names_from = "author", values_from = "total_usage") %>%
mutate(total_use = musk + satoshi) %>%
arrange(desc(total_use))) %>%
rename(nakamoto = satoshi)
print(stopword_freq, n = 12)
print(stopword_freq, n = 11, row.names = FALSE)
stopword_freq <- data %>%
group_by(author, word) %>%
summarise(total_usage = sum(freq)) %>%
pivot_wider(names_from = "author", values_from = "total_usage") %>%
mutate(total_use = musk + satoshi) %>%
arrange(desc(total_use)) %>%
rename(nakamoto = satoshi)
print(stopword_freq, n = 11, row.names = FALSE)
# Satoshi rates of the most popular words
data %>%
filter(author == "satoshi") %>%
right_join(stopword_freq %>%
filter(author == "satoshi") %>%
filter(row_number() < 7), by = "word") %>%
ggplot() +
geom_density(aes(per1000, group = word, color = word, fill = word), alpha = 0.3) +
theme_clean() +
labs(
title = "Rates of Use of Satoshi's Most Frequent Words"
)
# Satoshi rates of the most popular words
data %>%
dplyr::filter(author == "satoshi") %>%
right_join(stopword_freq %>%
filter(author == "satoshi") %>%
filter(row_number() < 7), by = "word") %>%
ggplot() +
geom_density(aes(per1000, group = word, color = word, fill = word), alpha = 0.3) +
theme_clean() +
labs(
title = "Rates of Use of Satoshi's Most Frequent Words"
)
View(data)
# Book graphic #1
data %>%
mutate(author = as.factor(author)) %>%
filter(word == "and") %>%
ggplot() +
geom_jitter(aes(x = per1000, y = author), color = "royalblue", height = 0.2) +
labs(
x = "Rate per 1000 words",
y = "Authorship",
title = "Observed usage rates of the word 'and'"
)
# Book graphic #1
data %>%
mutate(author = as.factor(author)) %>%
filter(word == "and") %>%
ggplot() +
geom_jitter(aes(x = rate, y = author), color = "royalblue", height = 0.2) +
labs(
x = "Rate per 1000 words",
y = "Authorship",
title = "Observed usage rates of the word 'and'"
)
data %>%
mutate(author = as.factor(author)) %>%
filter(word == "and") %>%
replace(satoshi, nakamoto) %>%
ggplot() +
geom_jitter(aes(x = rate, y = author), color = "royalblue", height = 0.2) +
labs(
x = "Rate per 1000 words",
y = "Authorship",
title = "Observed usage rates of the word 'and'"
)
data %>%
mutate(author = as.factor(author)) %>%
filter(word == "and") %>%
replace("satoshi", "nakamoto") %>%
ggplot() +
geom_jitter(aes(x = rate, y = author), color = "royalblue", height = 0.2) +
labs(
x = "Rate per 1000 words",
y = "Authorship",
title = "Observed usage rates of the word 'and'"
)
data %>%
mutate(author = as.factor(author)) %>%
filter(word == "and") %>%
replace(author, "satoshi", "nakamoto") %>%
ggplot() +
geom_jitter(aes(x = rate, y = author), color = "royalblue", height = 0.2) +
labs(
x = "Rate per 1000 words",
y = "Authorship",
title = "Observed usage rates of the word 'and'"
)
data %>%
mutate(author = as.factor(author)) %>%
filter(word == "and") %>%
replace("nakamoto", "satoshi") %>%
ggplot() +
geom_jitter(aes(x = rate, y = author), color = "royalblue", height = 0.2) +
labs(
x = "Rate per 1000 words",
y = "Authorship",
title = "Observed usage rates of the word 'and'"
)
data %>%
mutate(author = as.factor(author)) %>%
filter(word == "and") %>%
replace("nakamoto", "satoshi") %>%
ggplot() +
geom_jitter(aes(x = rate, y = author), color = "royalblue", height = 0.2) +
labs(
x = "Rate per 1000 words",
y = "Authorship",
title = "Observed usage rates of the word 'and'"
) +
theme_clean() +
theme(plot.title = element_text(hjust = 0.5))
data %>%
mutate(author = as.factor(author)) %>%
filter(word == "and") %>%
replace("nakamoto", "satoshi") %>%
ggplot() +
geom_jitter(aes(x = rate, y = author), color = "royalblue", height = 0.2) +
labs(
x = "Rate per 1000 words",
y = "Author",
title = "Observed usage rates of the word 'and'"
) +
theme_clean() +
theme(plot.title = element_text(hjust = 0.5))
data %>%
mutate(author = as.factor(author)) %>%
filter(word == "and") %>%
replace("nakamoto", "satoshi") %>%
ggplot() +
geom_jitter(aes(x = rate, y = author), color = "royalblue", height = 0.2) +
labs(
x = "Usage Rate (per 1000 words)",
y = "Author",
title = "Observed Usage Rates of the Word 'and'"
) +
theme_clean() +
theme(plot.title = element_text(hjust = 0.5))
data %>%
mutate(author = as.factor(author)) %>%
filter(word == "and") %>%
mutate(author = str_replace(author, "satoshi", "nakamoto")) %>%
ggplot() +
geom_jitter(aes(x = rate, y = author), color = "royalblue", height = 0.2) +
labs(
x = "Usage rate (per 1000 words)",
y = "Author",
title = "Observed usage rates of the word 'and'"
) +
theme_clean() +
theme(plot.title = element_text(hjust = 0.5))
?TeX
??TeX
library(latex2exp)
install.packages("latex2exp")
library(latex2exp)
ggplot(data.frame(x = c(0, 80)), aes(x)) +
stat_function(fun = dgamma, color = crcblue,
size = 1.5,
args = list(shape = 0.001, rate = 0.001)) +
ggplot2::annotate(geom = "text", x = 103, y = 0.04,
label = TeX("$(\\alpha,\\,  \\beta$) = (0.001, 0.001)"), size = 7 ) +
xlab(TeX("$\\lambda"))  +
ylab("Density")
ggplot(data.frame(x = c(0, 80)), aes(x)) +
stat_function(fun = dgamma, color = "blue",
size = 1.5,
args = list(shape = 0.001, rate = 0.001)) +
ggplot2::annotate(geom = "text", x = 103, y = 0.04,
label = TeX("$(\\alpha,\\,  \\beta$) = (0.001, 0.001)"), size = 7 ) +
xlab(TeX("$\\lambda"))  +
ylab("Density")
ggplot(data.frame(x = c(0, 80)), aes(x)) +
stat_function(fun = dgamma, color = "blue",
size = 1.5,
args = list(shape = 0.001, rate = 0.001)) +
ggplot2::annotate(geom = "text", x = 25, y = 0.025,
label = TeX("$(\\alpha,\\,  \\beta$) = (0.001, 0.001)"), size = 7 ) +
xlab(TeX("$\\lambda"))  +
ylab("Density")
ggplot(data.frame(x = c(0, 80)), aes(x)) +
stat_function(fun = dgamma, color = "blue",
size = 1.5,
args = list(shape = 0.001, rate = 0.001)) +
ggplot2::annotate(geom = "text", x = 25, y = 0.005,
label = TeX("$(\\alpha,\\,  \\beta$) = (0.001, 0.001)"), size = 7 ) +
xlab(TeX("$\\lambda"))  +
ylab("Density")
ggplot(data.frame(x = c(0, 80)), aes(x)) +
stat_function(fun = dgamma, color = "blue",
size = 1.5,
args = list(shape = 0.001, rate = 0.001)) +
ggplot2::annotate(geom = "text", x = 25, y = 0.005,
label = TeX("$(\\alpha,\\,  \\beta$) = (0.001, 0.001)"), size = 7 ) +
xlab(TeX("$\\lambda"))  +
ylab("Density") +
ylim(0, 0.01)
In order to formally test this hypothesis, I first identify an appropriate sampling distribution. I use the Poisson distribution as it is a popular sampling distribution for count or frequency data. I don't have a strong prior about the true rate of "amd" for either so I use a weak and rather non-informative gamma prior with parameters $\alpha = 0.001$ and $\beta = 0.001$. This prior is graphed below:
ggplot(data.frame(x = c(0, 80)), aes(x)) +
stat_function(fun = dgamma, color = "blue",
size = 1.5,
args = list(shape = 0.001, rate = 0.001)) +
ggplot2::annotate(geom = "text", x = 25, y = 0.005,
label = TeX("$(\\alpha,\\,  \\beta$) = (0.001, 0.001)"), size = 7 ) +
xlab(TeX("$\\lambda"))  +
ylab("Density") +
ylim(0, 0.01) +
ggtitle("Weak Prior") +
theme_clean() +
theme(plot.title = element_text(hjust = 0.5))
# Model string
modelString = "
model{
## sampling
for (i in 1:N) {
y[i] ~ dpois(n[i] * lambda / 1000)
}
## prior
lambda ~ dgamma(0.001, 0.001)
}
"
# Data
data1 <- data %>% filter(word == "the")
y <- data1$freq
n <- data1$total_words
the_data <- list("y" = y, "n" = n, N = length(y))
# Initial values
initsfunction <- function(chain){
.RNG.seed <- c(1,2)[chain]
.RNG.name <- c("base::Super-Duper",
"base::Wichmann-Hill")[chain]
return(list(.RNG.seed = .RNG.seed,
.RNG.name = .RNG.name))
}
# Simulated posterior draws
posterior <- run.jags(modelString,
data = the_data,
monitor = c("lambda"),
n.chains = 1,
burnin = 2000,
sample = 5000,
inits = initsfunction)
posterior
# Plot MCMC diagnostics
plot(posterior, vars = "lambda")
View(data1)
View(data)
modelString = "
model{
## sampling
for (i in 1:N) {
y[i] ~ dpois(n[i] * lambda / 1000)
}
## prior
lambda ~ dgamma(0.001, 0.001)
}
"
# Data
data1 <- data %>% filter(word == "and")
y <- data1$freq
n <- data1$total_words
the_data <- list("y" = y, "n" = n, N = length(y))
# Initial values
initsfunction <- function(chain){
.RNG.seed <- c(1,2)[chain]
.RNG.name <- c("base::Super-Duper",
"base::Wichmann-Hill")[chain]
return(list(.RNG.seed = .RNG.seed,
.RNG.name = .RNG.name))
}
# Simulated posterior draws
posterior <- run.jags(modelString,
data = the_data,
monitor = c("lambda"),
n.chains = 1,
burnin = 2000,
sample = 5000,
inits = initsfunction)
post <- as.mcmc(posterior)
plot(post)
# Plot MCMC diagnostics
plot(posterior, vars = "lambda")
# Plot MCMC diagnostics
plot(posterior)
```{r, warning = FALSE}
quartile(posterior)
quantile(as.mcmc(posterior$mcmc), c(.025, .5, .975))
posterior = run.jags(modelString,
data = the_data,
monitor = c("lambda"),
n.chains = 1,
burnin = 2000,
sample = 5000,
inits = initsfunction)
quantile(as.mcmc(posterior$mcmc), c(.025, .5, .975))
?as.mcmc
# Simulate lambda draws and distributions
one_rep <- function(i){
lambda <- post[i]
sd(rpois(length(y), n * lambda / 1000))
}
SD <- sapply(1:5000, one_rep)
# Plot simulated SD vs. observed SD
ggplot(data.frame(sd = SD), aes(sd)) +
geom_histogram(color = "black", fill = "white",
bins = 15) + theme(text = element_text(size = 18)) +
geom_vline(xintercept = sd(y), size = 3 ) +
ggplot2::annotate('text', x = 23, y = 950, label = "Observed", size = 5)
# Hypothesis test
(prob1 <- mean(SD >= sd(y))) # Reject null hypothesis
# 1: Specify prior and likelihood model
modelString = "
model{
## sampling
for (i in 1:N) {
y[i] ~ dpois(n[i] * lambda / 1000) # poisson sampling model
}
## prior
lambda ~ dgamma(0.001, 0.001) # gamma prior
}
"
# 2: Data
data1 <- data %>% filter(word == "the")
y <- data1$freq
n <- data1$total_words
the_data <- list("y" = y, "n" = n, N = length(y))
# Initial values for MCMC
initsfunction <- function(chain){
.RNG.seed <- c(1,2)[chain]
.RNG.name <- c("base::Super-Duper",
"base::Wichmann-Hill")[chain]
return(list(.RNG.seed = .RNG.seed,
.RNG.name = .RNG.name))
}
# Simulated posterior draws
posterior <- run.jags(modelString,
data = the_data,
monitor = c("lambda"),
n.chains = 1,
burnin = 2000,
sample = 5000,
inits = initsfunction)
post <- as.data.frame(as.mcmc(posterior))
quantile(as.mcmc(posterior$mcmc), c(.025, .5, .975))
# Plot MCMC diagnostics
plot(posterior)
# Simulate lambda draws and distributions
one_rep <- function(i){
lambda <- post[i]
sd(rpois(length(y), n * lambda / 1000))
}
SD <- sapply(1:5000, one_rep)
# 1: Specify prior and likelihood model
modelString = "
model{
## sampling
for (i in 1:N) {
y[i] ~ dpois(n[i] * lambda / 1000) # poisson sampling model
}
## prior
lambda ~ dgamma(0.001, 0.001) # gamma prior
}
"
# 2: Data
data1 <- data %>% filter(word == "the")
y <- data1$freq
n <- data1$total_words
the_data <- list("y" = y, "n" = n, N = length(y))
# Initial values for MCMC
initsfunction <- function(chain){
.RNG.seed <- c(1,2)[chain]
.RNG.name <- c("base::Super-Duper",
"base::Wichmann-Hill")[chain]
return(list(.RNG.seed = .RNG.seed,
.RNG.name = .RNG.name))
}
# Simulated posterior draws
posterior <- run.jags(modelString,
data = the_data,
monitor = c("lambda"),
n.chains = 1,
burnin = 2000,
sample = 5000,
inits = initsfunction)
post <- as.data.frame(as.mcmc(posterior))
quantile(as.mcmc(posterior$mcmc), c(.025, .5, .975))
# Plot MCMC diagnostics
plot(posterior)
# Simulate lambda draws and distributions
one_rep <- function(i){
lambda <- post[i]
sd(rpois(length(y), n * lambda / 1000))
}
SD <- sapply(1:5000, one_rep)
sapply(1:5000, one_rep) -> SD
SD <- sapply(1:5000, one_rep)
View(post)
# Hypothesis test
(prob1 <- mean(SD >= sd(y))) # Reject null hypothesis
(prob1 <- mean(SD >= sd(y)))
prob1 <- mean(SD >= sd(y))
# Simulate lambda draws and 5000 distributions
set.seed(())
# Simulate lambda draws and 5000 distributions
set.seed(9)
# Simulate lambda draws and 5000 distributions
set.seed(9) # for reproducibility
# Likelihood
data1 <- data %>% filter(word == "the")
y <- data1$freq
n <- data1$total_words
the_data <- list("y" = y, "n" = n, N = length(y))
# Initial values
initsfunction <- function(chain){
.RNG.seed <- c(1,2)[chain]
.RNG.name <- c("base::Super-Duper",
"base::Wichmann-Hill")[chain]
return(list(.RNG.seed = .RNG.seed,
.RNG.name = .RNG.name))
}
# Posterior draws
posterior <- run.jags(modelString,
data = the_data,
monitor = c("alpha", "beta", "mu"),
n.chains = 1,
burnin = 2000,
sample = 5000,
inits = initsfunction)
## sampling
for(i in 1:N){
# Prior
modelString = "
model{
## sampling
for(i in 1:N){
p[i] <- beta / (beta + n[i] / 1000)
y[i] ~ dnegbin(p[i], alpha)
}
## priors
mu <- alpha / beta
alpha ~ dgamma(.001, .001)
beta ~ dgamma(.001, .001)
}
"
